#!/usr/bin/env python

"""Simple script to visualize the results in a folder generated by the main script"""

import argparse
import math
import warnings
from typing import Any

import matplotlib.pyplot as plt
import torch
from botorch.models.transforms import outcome
from matplotlib.widgets import Slider

from human_bo import (
    conf,
    core,
    human_feedback_experiments,
    test_functions,
    utils,
    visualization,
)
from human_bo.core import pick_acqf


def get_x(res):

    # Expected for simple experiments (i.e. `scripts/run_human_ai_experiment.py`).
    if "initial_points" in res:
        return torch.cat(list(res["initial_points"]["x"].unsqueeze(1)) + res["query"])

    print("WARN: could not find `initial_points`, so just returning queries")
    return torch.cat(res["query"])


def get_y(res):

    # Expected for simple experiments (i.e. `scripts/run_human_ai_experiment.py`).
    if "initial_points" in res:
        return torch.cat(
            list(res["initial_points"]["y"].unsqueeze(0)) + res["feedback"]
        )

    print("WARN: could not find `initial_points`, so just returning feedback")
    return torch.cat(res["feedback"])


def get_regrets(res):
    return torch.Tensor([x["regret"] for x in res["evaluation_stats"]])


def get_y_max(res):
    return torch.Tensor([x["y_max"] for x in res["evaluation_stats"]])


def compare_regrets_over_time(files: list[str]) -> None:
    """Visualizes (plots) the regrets in of `files`

    Asserts that the results being compared have the same hyper parameter.

    :files: list of file paths (string)
    """
    experiment_config = torch.load(files[0], weights_only=True)["conf"]

    results: dict[str, Any] = {}
    for file in files:
        # Load the file and initiated configurations and results.
        new_results = torch.load(file, weights_only=True)
        new_conf = new_results["conf"]
        regrets = get_y_max(new_results)

        # Make sure experiment shares the same parameters.
        for k, c in conf.CONFIG.items():
            if "experiment-hyper-parameter" not in c["tags"]:
                continue  # We only check for "experiment-hyper-parameter" configurations

            assert (
                experiment_config[k] == new_conf[k]
            ), f"Different {k} parameters used: {experiment_config[k]} and {new_conf[k]}"

        # Store results.
        current_results_for_experiment = results
        for k, c in conf.CONFIG.items():
            if "experiment-parameter" not in c["tags"]:
                continue  # We store results per "experiment-parameter" combination.

            # dig into results
            try:
                current_results_for_experiment = current_results_for_experiment[
                    new_conf[k]
                ]
            except KeyError:
                current_results_for_experiment[new_conf[k]] = {}
                current_results_for_experiment = current_results_for_experiment[
                    new_conf[k]
                ]

        # `r` is now a bunch of results (one per seed)
        if current_results_for_experiment:
            current_results_for_experiment["regrets"] = torch.cat(
                (current_results_for_experiment["regrets"], regrets.unsqueeze(-1)),
                dim=1,
            )
        else:
            # First time looking at this particular experimental setup!
            current_results_for_experiment["conf"] = new_conf
            current_results_for_experiment["regrets"] = regrets.unsqueeze(-1)

    # Go over all results and compute (and store) their mean and standard error.
    for e in utils.recursively_filter_dict(
        results, lambda _, v: isinstance(v, dict) and "regrets" in v
    ):
        r = e["regrets"]
        e["mean_regret"] = r.mean(axis=1)
        # e["std_regret"] = 1.96 * r.std(axis=1) / math.sqrt(r.shape[1])
        e["std_regret"] = r.std(axis=1) / math.sqrt(r.shape[1])

    fig = plt.figure(figsize=(8, 6))

    for e in utils.recursively_filter_dict(
        results, lambda _, v: isinstance(v, dict) and "regrets" in v
    ):
        exp_params = e["conf"]
        mean = e["mean_regret"]
        std = e["std_regret"]

        plt.plot(
            range(len(mean)),
            mean,
            label=" ".join(
                conf.get_values_with_tag(exp_params, "experiment-parameter")
            ),
            linestyle="--",
        )

        plt.fill_between(
            range(len(mean)),
            mean - std,
            mean + std,
            alpha=0.2,
        )

        # Careful with the std bands when plotting in log scale (not symmetric)
        # plt.yscale("log")

    plt.xlabel("Budget (Iterations)")
    plt.ylabel("Simple regret")
    fig.legend(shadow=True)
    fig.tight_layout(pad=0)

    plt.show()


def visualize_trajectory_2D(result_file_content) -> None:
    """Visualizes the end result (approximation, sample data, etc)

    :file: file path (string)
    :returns: None
    """

    # Load configurations and results.
    exp_params = result_file_content["conf"]
    budget, n_init = exp_params["budget"], exp_params["n_init"]
    problem = test_functions.pick_test_function(exp_params["problem"], noise=0.0)

    # Pre-compute global variables.
    bounds = problem.bounds
    x1_min, x1_max, x2_min, x2_max = bounds.T.flatten()
    x1 = torch.linspace(x1_min, x1_max, 100)
    x2 = torch.linspace(x2_min, x2_max, 100)
    X1, X2 = torch.meshgrid(x1, x2, indexing="xy")
    X = torch.stack((X1, X2), dim=2)
    Y = problem(torch.stack((X1, X2), dim=2))

    queries, observations = get_x(result_file_content), get_y(result_file_content)

    # Generate results for each step.
    results = []
    for b in range(budget + 1):

        x, y = queries[: n_init + b], observations[: n_init + b]

        if len(x) == 0:
            assert n_init == 0 and b == 0
            # Weird corner case: there is nothing to plot
            results.append(
                {
                    "gpr_post_mean": torch.zeros_like(Y),
                    "gpr_post_mean_dist": torch.zeros_like(Y),
                    "gpr_post_var": torch.zeros_like(Y),
                    "x": x,
                    "y": y,
                    "acqf": torch.zeros_like(Y),
                }
            )

            continue

        gpr = core.fit_gp(
            x, y, core.pick_kernel(exp_params["kernel"], 1), input_bounds=bounds
        )

        posteriors = gpr.posterior(X)
        gpr_post_mean = posteriors.mean.squeeze()
        gpr_post_var = posteriors.variance.squeeze()

        acqf = pick_acqf(
            exp_params["acqf"],
            outcome.Standardize(m=1)(y.unsqueeze(-1))[0],
            gpr,
            bounds,
        )
        acqf_eval = acqf(X.reshape(-1, 2)[:, None, :]).reshape(gpr_post_mean.shape)

        results.append(
            {
                "gpr_post_mean": gpr_post_mean.detach().numpy(),
                "gpr_post_mean_dist": (gpr_post_mean - Y).detach().numpy(),
                "gpr_post_var": gpr_post_var.detach().numpy(),
                "x": x,
                "y": y,
                "acqf": acqf_eval.detach().numpy(),
            }
        )

    # Setup figures
    fig = plt.figure(figsize=(10, 8))
    surface_kwargs = {"rcount": 3, "ccount": 3, "lw": 0.5, "alpha": 0.3}
    contour_vals = {
        "var": "gpr_post_var",
        "mean_dist": "gpr_post_mean_dist",
        "acqf": "acqf",
    }
    axs = {
        "var": fig.add_subplot(2, 2, 1),
        "mean_dist": fig.add_subplot(2, 2, 2),
        "acqf": fig.add_subplot(2, 2, 3),
        "ax_3d": fig.add_subplot(2, 2, 4, projection="3d"),
    }
    contours = {
        k: axs[k].contourf(x1, x2, results[-1][v], cmap="binary")
        for k, v in contour_vals.items()
    }
    cbars = {k: plt.colorbar(contours[k]) for k in contour_vals}

    for k, ax in axs.items():
        ax.set_title(k)

    def draw_results(slider_input: float) -> int:
        """Draw results for budget=slider_input"""
        b = int(slider_input)

        r = results[b]
        x, y = r["x"], r["y"]

        for k, r_key in contour_vals.items():
            axs[k].contourf(x1, x2, r[r_key], cmap="binary")
            cbars[k].set_ticklabels(
                [
                    f"{number:.2f}"
                    for number in torch.linspace(
                        r[r_key].min(), r[r_key].max(), len(cbars[k].get_ticks())
                    ).tolist()
                ]
            )

            axs[k].scatter(x[:, 0], x[:, 1], color="green")

        axs["ax_3d"].clear()

        axs["ax_3d"].plot_surface(  # type: ignore
            X1.numpy(),
            X2.numpy(),
            Y.numpy(),
            color="green",
            edgecolor="green",
            label="f(x)",
            **surface_kwargs,
        )

        axs["ax_3d"].plot_surface(  # type: ignore
            X1.numpy(),
            X2.numpy(),
            r["gpr_post_mean"],
            color="blue",
            edgecolor="blue",
            label="GP mean",
            **surface_kwargs,
        )
        axs["ax_3d"].scatter(x[:, 0], x[:, 1], y, color="black")

        if b < budget:
            [next_x_1, next_x_2], next_y = queries[n_init + b], observations[n_init + b]

            for k in contour_vals:
                axs[k].scatter(next_x_1, next_x_2, color="orange")

            axs["ax_3d"].scatter(
                next_x_1, next_x_2, next_y, color="orange", label="next"
            )

        axs["ax_3d"].legend()
        fig.canvas.draw_idle()

        return 0

    fig.subplots_adjust(bottom=0.2)
    ax_budget = fig.add_axes((0.2, 0.05, 0.6, 0.03))

    slider_budget = Slider(
        ax_budget,
        "b",
        0,
        budget,
        valinit=budget,
        valstep=list(range(budget + 1)),
    )

    slider_budget.on_changed(draw_results)

    draw_results(budget)
    plt.show()


def visualize_trajectory_1D(results) -> None:
    """Visualizes the end result (approximation, sample data, etc)

    :file: file path (string)
    :returns: None
    """

    # Load configurations and results.
    exp_params = results["conf"]
    budget, n_init = exp_params["budget"], exp_params["n_init"]
    problem = test_functions.pick_test_function(exp_params["problem"], noise=0.0)

    bounds = torch.tensor(problem._bounds).T
    x_min, x_max = bounds.squeeze().tolist()
    x_linspace = torch.linspace(x_min, x_max, 101).reshape(-1, 1)
    y_truth = problem(x_linspace)
    queries, observations = get_x(results), get_y(results)

    # Get "global" (across all time steps) values.
    # optimal_xs = conf.CONFIG["problem"]["parser-arguments"]["choices"][
    #     exp_params["problem"]
    # ]["optimal_x"]
    # user_models = [
    #     human_feedback_experiments.pick_user_model(
    #         exp_params["user_model"], optimal_x, problem
    #     )(x_linspace, y_truth)
    #     for optimal_x in optimal_xs
    # ]

    # Process results for each "time step"
    results = []
    for b in range(budget + 1):
        x, y = queries[: n_init + b], observations[: n_init + b]

        if len(x) == 0:

            # Little hack: very rarely we start experiments with no initial points.
            # In this case, we _cannot_ compute any of the things we want to do below.
            # So we just return zero for all of them.
            results.append(
                {
                    "gpr_post_mean": torch.zeros(len(x_linspace)),
                    "gpr_post_var": torch.zeros(len(x_linspace)),
                    "x": x,
                    "y": y,
                    "acqf": torch.zeros(len(x_linspace)),
                }
            )

            continue

        gpr = core.fit_gp(
            x, y, core.pick_kernel(exp_params["kernel"], 1), input_bounds=bounds
        )

        posteriors = gpr.posterior(x_linspace)
        gpr_post_mean = posteriors.mean.squeeze().detach().numpy()
        gpr_post_var = posteriors.variance.squeeze().detach().numpy()

        acqf = pick_acqf(
            exp_params["acqf"],
            outcome.Standardize(m=1)(y.unsqueeze(-1))[0],
            gpr,
            bounds,
        )
        acqf_eval = acqf(x_linspace[:, None, :]).detach().numpy()

        results.append(
            {
                "gpr_post_mean": gpr_post_mean,
                "gpr_post_var": gpr_post_var,
                "x": x,
                "y": y,
                "acqf": acqf_eval,
            }
        )

    # Create our plot
    fig = plt.figure(figsize=(10, 8))
    ax = fig.gca()

    def draw_results(slider_input: float) -> int:
        """Actually draws the results in our figure"""
        ax.clear()

        b = int(slider_input)

        # Grab results for time step `b`
        r = results[b]
        m, var, x, y, acqf = (
            r["gpr_post_mean"],
            r["gpr_post_var"],
            r["x"],
            r["y"],
            r["acqf"],
        )

        # Plot global
        # for user_model in user_models:
        #     ax.plot(x_linspace, user_model, "g", label="User model")

        ax.plot(x_linspace, y_truth, label="Ground Truth")

        # Plot results
        ax.plot(x_linspace, m, "b", label="GP Mean function")
        ax.fill_between(
            x_linspace.squeeze(),
            m - var,
            m + var,
            alpha=0.2,
            color="b",
        )
        ax.scatter(x, y, alpha=0.5, color="black", marker="x", s=100)

        if b < budget:
            ax.scatter(
                queries[n_init + b], observations[n_init + b], color="r", label="Next"
            )

        ax.plot(
            x_linspace,
            acqf,
            linestyle="dotted",
            color="orange",
            linewidth=3,
            label=exp_params["acqf"],
        )

        # Basic plotting style
        ax.set_xlabel("x")
        ax.set_ylabel("y")
        ax.legend(shadow=True)
        ax.set_title(
            "_".join(
                conf.get_values_with_tag(exp_params, "experiment-parameter")
                + [str(exp_params["seed"])]
            )
        )

        fig.canvas.draw_idle()

        return 0

    fig.subplots_adjust(bottom=0.2)
    ax_budget = fig.add_axes((0.2, 0.05, 0.6, 0.03))

    slider_budget = Slider(
        ax_budget,
        "b",
        0,
        budget,
        valinit=budget,
        valstep=list(range(budget + 1)),
    )

    slider_budget.on_changed(draw_results)

    draw_results(budget)
    plt.show()


if __name__ == "__main__":
    warnings.showwarning = utils.warn_with_traceback

    conf.CONFIG.update(human_feedback_experiments.CONFIG)

    parser = argparse.ArgumentParser(description="Command description.")
    parser.add_argument(
        "-t",
        "--type",
        default="regrets",
        type=str,
        help="Type of visualization.",
        choices=["regrets", "trajectory"],
    )
    parser.add_argument(
        "-f",
        "--files",
        nargs="*",
        type=str,
        help="All files that need to be processed.",
    )
    parser.add_argument(
        "--budget",
        type=int,
        help="For which budget to plot",
        default=True,
        action=argparse.BooleanOptionalAction,
    )

    args = parser.parse_args()

    # Basic setup for all visualizations.
    torch.set_default_dtype(torch.double)
    visualization.set_matplotlib_params()

    if args.type == "regrets":
        compare_regrets_over_time(args.files)

    if args.type == "trajectory":
        if len(args.files) != 1:
            raise ValueError("Please only provide 1 file when plotting trajectory")

        file_content = torch.load(args.files[0], weights_only=True)

        dim_test_func = conf.CONFIG["problem"]["parser-arguments"]["choices"][
            file_content["conf"]["problem"]
        ]["dims"]

        if dim_test_func == 1:
            visualize_trajectory_1D(file_content)
        elif dim_test_func == 2:
            visualize_trajectory_2D(file_content)
        else:
            raise ValueError(
                f"Experiment on {file_content['conf']['problem']} is too high-dimensional ({dim_test_func}) to visualize"
            )
