"""Simple script to visualize the results in a folder generated by the main script"""

from botorch.fit import fit_gpytorch_model
import numpy as np
from gpytorch.mlls import ExactMarginalLogLikelihood
import math
from botorch.models import SingleTaskGP
import matplotlib.pyplot as plt
import torch
import argparse
from human_bo.conf import CONFIG
from human_bo.factories import pick_kernel, pick_oracle, pick_test_function
from human_bo.utils import recursively_filter_dict

from human_bo.visualization import set_matplotlib_params


def compare_regrets_over_time(files: list[str]) -> None:
    """Visualizes (plots) the regrets in of `files`

    Asserts that the results being compared have the same hyper parameter.

    :files: list of file paths (string)
    """
    experiment_config = torch.load(files[0])["conf"]

    results = {}
    for file in files:
        # Load the file.
        new_results = torch.load(file)
        new_conf = new_results["conf"]
        regrets = new_results["regrets"]

        # Make sure experiment shares the same parameters.
        for k, c in CONFIG.items():
            if "experiment-hyper-parameter" not in c["tags"]:
                continue  # We only check for "experiment-hyper-parameter" configurations

            assert (
                experiment_config[k] == new_conf[k]
            ), f"Different {k} parameters used: {experiment_config[k]} and {new_conf[k]}"

        # Store results.
        current_results_for_experiment = results
        for k, c in CONFIG.items():
            if "experiment-parameter" not in c["tags"]:
                continue  # We store results per "experiment-parameter" combination.

            # dig into results
            try:
                current_results_for_experiment = current_results_for_experiment[
                    new_conf[k]
                ]
            except KeyError:
                current_results_for_experiment[new_conf[k]] = {}
                current_results_for_experiment = current_results_for_experiment[
                    new_conf[k]
                ]

        # `r` is now a bunch of results (one per seed)
        if current_results_for_experiment:
            current_results_for_experiment["regrets"] = torch.cat(
                (current_results_for_experiment["regrets"], regrets.unsqueeze(-1)),
                dim=1,
            )
        else:
            # First time looking at this particular experimental setup!
            current_results_for_experiment["conf"] = new_conf
            current_results_for_experiment["regrets"] = regrets.unsqueeze(-1)

    # Go over all results and compute (and store) their mean and standard error.
    for e in recursively_filter_dict(
        results, lambda _, v: isinstance(v, dict) and "regrets" in v
    ):
        r = e["regrets"]
        e["mean_regret"] = r.mean(axis=1)
        e["std_regret"] = 1.96 * r.std(axis=1) / math.sqrt(r.shape[1])

    fig = plt.figure()
    for e in recursively_filter_dict(
        results, lambda _, v: isinstance(v, dict) and "regrets" in v
    ):
        conf = e["conf"]
        mean = e["mean_regret"]
        std = e["std_regret"]

        plt.plot(
            range(len(mean)),
            mean,
            label=" ".join(
                [
                    v
                    for k, v in conf.items()
                    if "experiment-parameter" in CONFIG[k]["tags"]
                ]
            ),
            linestyle="--",
        )

        plt.fill_between(
            range(len(mean)),
            mean - std,
            mean + std,
            alpha=0.2,
        )  # careful with the std bands when plotting in log scale (nonsymmetric)
        plt.yscale("log")

    fig.legend(shadow=True)
    fig.supxlabel("Budget (Iterations)")
    fig.supylabel("Simple regret")

    plt.tight_layout()
    plt.show()


def visualize_end_result(files: list[str]) -> None:
    """Visualizes the end result (approximation, sample data, etc)

    :files: list of file paths (string)
    :returns: None
    """

    # Pre-compute some constants
    x_truth = torch.linspace(0.0, 1.0, 101).reshape(-1, 1)
    candidate_test_functions = [
        f for f, c in CONFIG["function"]["choices"].items() if c["dims"] == 1
    ]

    for file in files:
        # Load the file.
        new_results = torch.load(file)
        conf = new_results["conf"]

        if conf["function"] not in candidate_test_functions:
            print(
                f"{file} is not an 1-dimensional experiment (in {candidate_test_functions}) and thus is excluded"
            )
            continue

        problem = pick_test_function(conf["function"])
        y_truth = problem(x_truth)

        # Process results
        x, y = new_results["train_X"], new_results["train_Y"]

        kernel = pick_kernel(conf["kernel"], 1)
        gpr = SingleTaskGP(x, y, covar_module=kernel)
        mll = ExactMarginalLogLikelihood(gpr.likelihood, gpr)
        fit_gpytorch_model(mll, max_retries=10)

        gpr_post_mean = gpr.likelihood(gpr(x_truth)).mean.detach().numpy()
        gpr_post_var = np.sqrt(gpr.likelihood(gpr(x_truth)).variance.detach().numpy())

        plt.figure()

        for optimal_x in CONFIG["function"]["choices"][conf["function"]]["optimal_x"]:
            oracle = pick_oracle(conf["oracle"], optimal_x, problem)(x_truth, y_truth)
            plt.plot(x_truth, oracle, "g", label="Oracle function")

        plt.plot(x_truth, gpr_post_mean, "b", label="GP Mean function")
        plt.fill_between(
            x_truth.squeeze(),
            gpr_post_mean - gpr_post_var,
            gpr_post_mean + gpr_post_var,
            alpha=0.2,
            color="b",
        )

        # Plot figure
        plt.plot(x_truth, y_truth, label="Ground Truth")
        plt.scatter(x, y, alpha=0.5, color="black", marker="x", s=100)

        plt.ylabel("$y$")
        plt.xlabel("$x$")
        plt.legend(shadow=True)

        plt.title(
            " ".join(
                [
                    v
                    for k, v in conf.items()
                    if "experiment-parameter" in CONFIG[k]["tags"]
                ]
            )
        )
        plt.tight_layout()

    plt.show()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Command description.")
    parser.add_argument(
        "-f",
        "--files",
        nargs="*",
        type=str,
        help="All files that need to be processed",
    )
    parser.add_argument(
        "-t",
        "--type",
        default="regrets",
        type=str,
        help="Type of visualization",
        choices=["regrets", "end-result"],
    )
    args = parser.parse_args()

    # Basic setup for all visualizations.
    torch.set_default_dtype(torch.double)
    set_matplotlib_params()

    if args.type == "regrets":
        compare_regrets_over_time(args.files)
    if args.type == "end-result":
        visualize_end_result(args.files)
